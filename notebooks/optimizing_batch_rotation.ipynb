{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing rotation during training\n",
    "* Goal is to make a network that will represent all images at some cannonical orientation while being trained on variable orientation images.\n",
    "* This should result in an overall more simple model able to do the same work.\n",
    "* Switch between optimizing the model parameters, and optimizing the network.\n",
    "* Could also do it on one batch at a fixed orientation first, but this is much less cool so we won't\n",
    "\n",
    "For each batch:\n",
    "1. Optimize the rotation parameters (randomly), pick the best, couple SGD steps.\n",
    "2. Do the backward pass to update the weights.\n",
    "\n",
    "Could iterate this process for a single batch, gradually getting the \"best\" representation but instead we will hope that over the course of training it will learn it for them all at once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "1. Clean this and other notebook\n",
    "2. Write a bunch of stuff (not exactly sure how to talk about this new addition, if it works). Guess:\n",
    "\n",
    "**Outline**\n",
    "We create a method to make VAEs generalize to distribtional shifts in the form of affine transformations, and show experimental results with this method for rotations (and maybe shears). We show that existing methods for enforcing this through data augmentation force an increase in model complexity, or explicit equivariance which results a dramatic increase in memory use and is difficult to scale to more transforms, or are approximate methods, prone to error (STN). This method takes a different approach, where instead of increasing model complexity, increases computational complexity through optimizing the transform to some cannonical orientation.\n",
    "\n",
    "**ABS/intro**\n",
    "We propose an alternative method to enable models to generalize to affine transfotmations. We show that data augmentation alone has limitations. We show a method for enabling VAEs trained on a single orientation (a subset of the possible distribution) to generalize, as well as a way to train the VAE on the full distribution, without increasing the model capicity.\n",
    "\n",
    "The model capicity required to work for this subset of the possible distribution (rotations of 0 degrees) is less than that required for a model expected to generalize to the more broader distribution of all possible rotations. This means that naively using data augmentation should increase the model capacity required.\n",
    "\n",
    "\n",
    "**Generalizing to Rotations**\n",
    "Given a VAE trained on images at one orientation $$r_0$$, we can make this model generalize to other rotations taking an image at some rotation, and finding the rotation that minimizes the reconstruction error. This is implemented as: (from paper)\n",
    "\n",
    "But this procedure requires the images to be supplied at one given orientation, otherwise this model will devolve to the case of just doing data augmenetation, with the increased model capacit required. It would be better to have the model be able to be trained on a dataset without this restriction. \n",
    "\n",
    "**Rotation Training**\n",
    "We use MNIST with random rotations. (or would it have been better to use fixed rotations? probably the same, because we ignore rotation when we train anyway through the random rotation search).\n",
    "\n",
    "As an alternative to this, we use a procedure where we repeatedly optimize the rotation angle of each image during the training process. The goal is to have the VAE learn some orientation for each class of image that will minimize the loss for a given model complexity. It is not necessarily the case that the best way is to have all stored in the standard orientation we view images. We formulate this in terms of an optimization process added into the training loop, where for each batch of images the rotation of each image is optimized to reduce the model reconstruction loss before backpropagation is done. so overall thee training process is:\n",
    "\n",
    "for each batch:\n",
    "* try 20 random rotations $r_opt$, pick the one giving the lowest VAE loss\n",
    "* do SGD on the rotation with lowest loss\n",
    "* given this loss $r_opt$, perform standard backprop on the VAE.\n",
    "\n",
    "This is implemented naively, with no attempt to be efficient. It is likely that it would be more useful to optimize per batch, but these were not tested. Another much better approach would be to combine an STN in the optimization process, as although this is not always correct, it could be much faster than simple random search. Process of use STN, eval, use STN again, ..., but mixed in with randomness and SGD. This way you get the benefit of explicitly optimizing the objective, with faster speed than the random way.\n",
    "\n",
    "In this way we enable models to be equivariant to rotation through the use of optimization, as an alternative to explicting encoding this into the model (taco) or approximating it (stn). While in this current form the model is practically limited, it could be useful to use this idea of optimizing a representation over some set of transfroms to enforce generalization.\n",
    "\n",
    "5. Add couple notes to github."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "Minimial model size through self-supervised generalization.\n",
    "\n",
    "\n",
    "We have no problem understanding what this is:\n",
    "[mnist]\n",
    "\n",
    "Same with this:\n",
    "[rot mnist]\n",
    "\n",
    "But convolutional neural networks do not generalize. Look at the loss when MNIST digits are encoded at different rotations:\n",
    "[loss graph]\n",
    "\n",
    "Why is it that for a machine learning model to generalize to a rotated image, the most popular method is to feed it the same image at a bunch of different rotations?\n",
    "[batch of rot mnist]\n",
    "\n",
    "This rotation augmented method seems strange. Are we really going to force the model to learn a totally different representation for each rotation? It would be niceeer if the model would learn some abstract representation, and then perform some \"mental rotation\", as was discussed by goeff hinton in [\"What is wrong with convolutional neural nets ?\"](http://www.youtube.com/watch?v=rTawFwUvnLE&t=19m50s)\n",
    "\n",
    "In addition to this being unappealing, we can see that if you blindly do data augmentation, greater model complexity is required. We would prefer this to learn a representation with only a single rotation parameter, but it turns out you need to increase the latent loss a lot for this to work.??? I didn this wrong, adding in KL diveregence with multiple variables???\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "One popular approach, the [Spatial Transformer Network](https://arxiv.org/abs/1506.02025) attempts to solve this problem by applying a learned affine transform to return the image to its original orientation. As long as the network figures out the correct transform, this is exactly the solution!\n",
    "\n",
    "The issue happens when the model doesn't give the correct transform:\n",
    "[img at 150 -> img at 45]. \n",
    "We would like to iteratively move the image closer and closer to this \"cannonical\" orientation, based on how \"weird\" the image looks to us.\n",
    "\n",
    "But in classification, there is no good metric to measure how weird an image is. The best we can do is something like the classification difficulty/entropy of softmax output.\n",
    "\n",
    "But when we have a VAE, there's a clear metric for how \"weird\" an image is, the loss!\n",
    "\n",
    "So what if when given an image, we optimize the rotation to minimize this loss. We can add an affine transform layer before and after the vae, \n",
    "\n",
    "[vae arch]\n",
    "\n",
    "In this form the whole network is nice and differentiable. So given an image, we can optimize $\\theta$ using SDG, the same way we optimize the parameters of the network! In practice, there's some local minima, so some random restarts is required.\n",
    "\n",
    "And using this, we easily generalize to rotations, and have traded off using an increased model complexity for using increased compute.\n",
    "\n",
    "[graph of performance on rot with and without affine. (should compare to with rotation augmentation)]\n",
    "\n",
    "Note that this requires all images to be given to the model in some cannonical orientation at training time. This isn't good! People don't need this, and many datasets don't have this. How can we get around this, and create both a minimal representation by forcing the model to encode only one [basis? subset single otientation single direction fo variance. ] of the dataset during training? If we use data augmentation, we'll force the model to ineffeiciently be able to fully encode all variantion across $\\theta$.\n",
    "\n",
    "An option is to iteratively optimize the transform parameters, $\\theta$ **before** SGD is done, for each batch of images. The intuition is that because it is optimal for the model to only learn to encode at a subset of the rotations / single rotation, as training happens the model will gradually start to learn to encode at some rotations better than others, and eventually the model will only be training on a single rotation.\n",
    "[graph]\n",
    "[explaination of graph beecause its complicated]\n",
    "It worked!\n",
    "\n",
    "\n",
    "[possibly insert the reeduction in variace in angle graph]\n",
    "\n",
    "So we've got an autoencoder that generalizes to roatation using computation instead of increasing model size!\n",
    "\n",
    "Here we focused on rotations, but this can easily be extended by changing the affine transform matrix form [] to [].\n",
    "\n",
    "\n",
    "____________\n",
    "\n",
    "Note: *Others have realized this too, and other approaches to this problem have included [explicitly encoding for all rotation angles](http://proceedings.mlr.press/v48/cohenc16.pdf), which increases model complexity greatly*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "from IPython.display import display\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "from utils.display import read_img_to_np, torch_to_np\n",
    "from utils.norms import MNIST_norm\n",
    "import model.model as module_arch\n",
    "import data_loader.data_loaders as module_data\n",
    "import model.loss as module_loss\n",
    "import model.metric as module_metric\n",
    "from model.model import AffineVAE\n",
    "from data_loader.data_loaders import make_generators_MNIST, make_generators_MNIST_CTRNFS\n",
    "\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_loaders_config(PATH, old_gpu='cuda:0', new_gpu='cuda:1'):\n",
    "    \"\"\"PATH: path to dir where training results of a run are saved\"\"\"\n",
    "    PATH = Path(PATH)\n",
    "    config_loc = PATH / 'config.json'\n",
    "    weight_path = PATH / 'model_best.pth'\n",
    "    config = json.load(open(config_loc))\n",
    "    \n",
    "    \n",
    "    def get_instance(module, name, config, *args):\n",
    "        return getattr(module, config[name]['type'])(*args, **config[name]['args'])\n",
    "\n",
    "    data_loader = get_instance(module_data, 'data_loader', config)['train']\n",
    "    valid_data_loader = get_instance(module_data, 'data_loader', config)['val']\n",
    "    model = get_instance(module_arch, 'arch', config)\n",
    "    model = model.to(torch.device(new_gpu))\n",
    "    checkpoint = torch.load(weight_path, map_location={'cuda:0': 'cuda:1'})\n",
    "    state_dict = checkpoint['state_dict']\n",
    "    \n",
    "    if config['n_gpu'] > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    model.load_state_dict(state_dict)\n",
    "    model = model.to(device).eval()\n",
    "    \n",
    "    loss_fn = get_instance(module_loss, 'loss', config)\n",
    "    metric_fns = [getattr(module_metric, met) for met in config['metrics']]\n",
    "    \n",
    "    return model, data_loader, valid_data_loader, loss_fn, metric_fns, config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_to_size(img, new_size):\n",
    "    delta_width = new_size - img.size()[1]\n",
    "    delta_height = new_size - img.size()[2]\n",
    "    pad_width = delta_width //2\n",
    "    pad_height = delta_height //2\n",
    "    img = F.pad(img, (pad_height, pad_height, pad_width, pad_width), 'constant', 0)\n",
    "    return img\n",
    "\n",
    "def rotate_mnist_batch(x, return_size=40, fixed_rotation=None):\n",
    "    \"\"\"Rotate batch without squishing the img. Pad all imgs to same size\"\"\"\n",
    "    batch_size = x.shape[0]\n",
    "    rot_x = torch.zeros((batch_size, 1, return_size, return_size))\n",
    "    for i in range(batch_size):\n",
    "        img = TF.to_pil_image(x[i, :, :])\n",
    "        if fixed_rotation:\n",
    "            img = TF.rotate(img, fixed_rotation)\n",
    "\n",
    "        img = transforms.ToTensor()(img)\n",
    "        if return_size:\n",
    "            img = pad_to_size(img, return_size)\n",
    "        # MNIST norm, wrong because imgs are padded\n",
    "        img = transforms.Normalize((0.1307,), (0.3081,))(img)\n",
    "        rot_x[i, :, :, :] = img\n",
    "    return rot_x\n",
    "\n",
    "def AFFINE_MNIST_rot_perf(affine_model, data_loader, loss_fn, device, fixed_rotation, \n",
    "                          optimize=False, iterations=0, num_rand_restarts=200, num_imgs=1000):\n",
    "    \"\"\"Evaluate performance on MNIST Dataset using a given rotation\n",
    "    Dataloader should be MNISTCustomTRNFS with size=28x28, unnormalized, not rotated\"\"\"\n",
    "\n",
    "    with torch.cuda.device(device.index):\n",
    "        affine_model = affine_model.to(device)\n",
    "        affine_model.eval()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, (data, target) in enumerate(data_loader):\n",
    "                batch_size = data.shape[0]\n",
    "                rot_x = rotate_mnist_batch(data, return_size=40, fixed_rotation=fixed_rotation)\n",
    "                rot_x, target = rot_x.to(device), target.to(device)\n",
    "                if optimize:\n",
    "                    best_affine, loss = affine_model.optimize_rotation(rot_x, num_times=num_rand_restarts, \n",
    "                                                                       iterations=iterations)\n",
    "                else:\n",
    "                    output = affine_model(rot_x, deterministic=True, theta=0.0)\n",
    "                    loss = loss_fn(output, rot_x).item()\n",
    "\n",
    "                total_loss += loss * batch_size\n",
    "                if i>num_imgs:\n",
    "                    break\n",
    "\n",
    "        n_samples = len(data_loader.sampler)\n",
    "        return total_loss / num_imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance of Batch-Optimized VAE\n",
    "* Comparing the rotation optimized VAE to the one on random rotations to the one trained at a single orientation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rotation: 0\n"
     ]
    }
   ],
   "source": [
    "files_dict_loc = '/media/rene/data/MNIST/files_dict.pkl'\n",
    "data_loaders = make_generators_MNIST_CTRNFS(files_dict_loc, batch_size=1, num_workers=4, \n",
    "                                            return_size=28, rotation_range=None, normalize=False)\n",
    "                                            \n",
    "config_loc = '/media/rene/data/equivariance/mnist/vae_mnist_L8/0129_171505'\n",
    "VAE, data_loader, valid_data_loader, loss_fn, metric_fns, config = get_model_loaders_config(config_loc, old_gpu='cuda:1', new_gpu='cuda:0')\n",
    "VAE = VAE.to(device)\n",
    "AffineVAE = getattr(module_arch, 'AffineVAE')\n",
    "affine_model_single_rot = AffineVAE(pre_trained_VAE=VAE, img_size=28, input_dim=1, output_dim=1, latent_size=8, use_STN=False)\n",
    "affine_model_single_rot = affine_model_single_rot.to(device)\n",
    "                                                            \n",
    "config_loc= '/media/rene/data/equivariance/mnist/vae_mnist_L8_rotate/0204_191133'\n",
    "VAE, data_loader, valid_data_loader, loss_fn, metric_fns, config = get_model_loaders_config(config_loc, old_gpu='cuda:1', new_gpu='cuda:0')\n",
    "VAE = VAE.to(device)\n",
    "AffineVAE = getattr(module_arch, 'AffineVAE')\n",
    "affine_model_rand_rot = AffineVAE(pre_trained_VAE=VAE, img_size=28, input_dim=1, output_dim=1, latent_size=8, use_STN=False)\n",
    "affine_model_rand_rot = affine_model_rand_rot.to(device)\n",
    "                                            \n",
    "config_loc = '/media/rene/data/equivariance/mnist/batch_avae_mnist_L8/0404_145427'\n",
    "affine_model_opt_rot, data_loader, valid_data_loader, loss_fn, metric_fns, config = get_model_loaders_config(config_loc, old_gpu='cuda:1', new_gpu='cuda:0')\n",
    "affine_model_opt_rot = affine_model_opt_rot.to(device)\n",
    "# AffineVAE = getattr(module_arch, 'AffineVAE')\n",
    "# affine_model_opt_rot = AffineVAE(pre_trained_VAE=VAE, img_size=28, input_dim=1, output_dim=1, latent_size=8, use_STN=False)\n",
    "# affine_model_opt_rot = affine_model_opt_rot.to(device)\n",
    "                                            \n",
    "\n",
    "results = pd.DataFrame()\n",
    "num_imgs = 500\n",
    "\n",
    "for rotation in range(0, 180, 15):\n",
    "    print(f'rotation: {rotation}')\n",
    "    log = {}\n",
    "    log['rotation'] = rotation\n",
    "\n",
    "    log['loss_single_rot_noopt'] = AFFINE_MNIST_rot_perf(affine_model_single_rot, data_loaders['val'], loss_fn, device,\n",
    "                                                         fixed_rotation=rotation, optimize=False, iterations=0, \n",
    "                                                         num_rand_restarts=1, num_imgs=num_imgs)\n",
    "\n",
    "    log['loss_single_rot'] = AFFINE_MNIST_rot_perf(affine_model_single_rot, data_loaders['val'], loss_fn, device,\n",
    "                                                   fixed_rotation=rotation, optimize=True, iterations=20, \n",
    "                                                   num_rand_restarts=20, num_imgs=num_imgs)\n",
    "    \n",
    "    log['loss_rand_rot'] = AFFINE_MNIST_rot_perf(affine_model_rand_rot, data_loaders['val'], loss_fn, device, \n",
    "                                                 fixed_rotation=rotation, optimize=True, iterations=20, \n",
    "                                                 num_rand_restarts=20, num_imgs=num_imgs)\n",
    "\n",
    "    log['loss_opt_rot'] = AFFINE_MNIST_rot_perf(affine_model_opt_rot, data_loaders['val'], loss_fn, device, \n",
    "                                                fixed_rotation=rotation, optimize=True, iterations=20, \n",
    "                                                num_rand_restarts=20, num_imgs=num_imgs)\n",
    "    \n",
    "    results = results.append(log, ignore_index=True)\n",
    "\n",
    "results.to_csv('/media/rene/code/equivariance/results/affine_L8_rot_compare_all_sgd20_r40.csv')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(results['rotation'], results['loss_single_rot_noopt'], label=\"VAE\")\n",
    "ax.plot(results['rotation'], results['loss_single_rot'], label=\"AVAE on single orientation\")\n",
    "ax.plot(results['rotation'], results['loss_rand_rot'], label=\"AVAE on random rotations\")\n",
    "ax.plot(results['rotation'], results['loss_opt_rot'], label=\"AVAE optimized for each rotation\")\n",
    "\n",
    "\n",
    "ax.set(xlabel='Rotation', ylabel='Loss')\n",
    "ax.legend(loc='best')\n",
    "\n",
    "plt.savefig('/media/rene/code/equivariance/imgs/affine_L8_rot_compare_all_sgd20_r20.png', bbox='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = Image.open('/media/rene/code/equivariance/imgs/affine_L8_rot_compare_all_sgd20_r40.png')\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HVAE",
   "language": "python",
   "name": "hvae"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
