{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing rotation during training\n",
    "* Goal is to make a network that will represent all images at some cannonical orientation while being trained on variable orientation images.\n",
    "* This should result in an overall more simple model able to do the same work.\n",
    "* Switch between optimizing the model parameters, and optimizing the network.\n",
    "* Could also do it on one batch at a fixed orientation first, but this is much less cool so we won't\n",
    "\n",
    "For each batch:\n",
    "1. Optimize the rotation parameters (randomly), pick the best, couple SGD steps.\n",
    "2. Do the backward pass to update the weights.\n",
    "\n",
    "Could iterate this process for a single batch, gradually getting the \"best\" representation but instead we will hope that over the course of training it will learn it for them all at once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "1. Check Results - just like in other notebook, run examples, sample from vae, etc. \n",
    "2. Do standard graph of loss vs. rotation, comparing this vae to one trained on rotation augmented images. Hopefully we find that this one is better for a given latent size. Optimize both! the idea is that the other model should have learned to optimize more for other rotations, and so won't do as well on the standard ones. Big if.\n",
    "3. Clean this and other notebook\n",
    "4. Write a bunch of stuff (not exactly sure how to talk about this new addition, if it works). Guess:\n",
    "\n",
    "**Outline**\n",
    "\n",
    "\n",
    "**ABS/intro**\n",
    "We propose an alternative method to enable models to generalize to affine transfotmations. We show that data augmentation alone has limitations. We show a method for enabling VAEs trained on a single orientation (a subset of the possible distribution) to generalize, as well as a way to train the VAE on the full distribution, without increasing the model capicity.\n",
    "\n",
    "The model capicity required to work for this subset of the possible distribution (rotations of 0 degrees) is less than that required for a model expected to generalize to the more broader distribution of all possible rotations. This means that naively using data augmentation should increase the model capacity required.\n",
    "\n",
    "\n",
    "**Generalizing to Rotations**\n",
    "Given a VAE trained on images at one orientation $$r_0$$, we can make this model generalize to other rotations taking an image at some rotation, and finding the rotation that minimizes the reconstruction error. This is implemented as: (from paper)\n",
    "\n",
    "But this procedure requires the images to be supplied at one given orientation, otherwise this model will devolve to the case of just doing data augmenetation, with the increased model capacit required. It would be better to have the model be able to be trained on a dataset without this restriction. \n",
    "\n",
    "**Rotation Training**\n",
    "We use MNIST with random rotations. (or would it have been better to use fixed rotations? probably the same, because we ignore rotation when we train anyway through the random rotation search).\n",
    "\n",
    "As an alternative to this, we use a procedure where we repeatedly optimize the rotation angle of each image during the training process. The goal is to have the VAE learn some orientation for each class of image that will minimize the loss for a given model complexity. It is not necessarily the case that the best way is to have all stored in the standard orientation we view images. We formulate this in terms of an optimization process added into the training loop, where for each batch of images the rotation of each image is optimized to reduce the model reconstruction loss before backpropagation is done. so overall thee training process is:\n",
    "\n",
    "for each batch:\n",
    "* try 20 random rotations $$r_opt$$, pick the one giving the lowest VAE loss\n",
    "* do SGD on the rotation with lowest loss\n",
    "* given this loss $$r_opt$$, perform standard backprop on the VAE.\n",
    "\n",
    "This is implemented naively, with no attempt to be efficient. It is likely that it would be more useful to optimize per batch, but these were not tested. Another much better approach would be to combine an STN in the optimization process, as although this is not always correct, it could be much faster than simple random search. Process of use STN, eval, use STN again, ..., but mixed in with randomness and SGD. This way you get the benefit of explicitly optimizing the objective, with faster speed than the random way.\n",
    "\n",
    "In this way we enable models to be equivariant to rotation through the use of optimization, as an alternative to explicting encoding this into the model (taco) or approximating it (stn). While in this current form the model is practically limited, it could be useful to use this idea of optimizing a representation over some set of transfroms to enforce generalization.\n",
    "\n",
    "5. Add couple notes to github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "from IPython.display import display\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "from utils.display import read_img_to_np, torch_to_np\n",
    "from utils.norms import MNIST_norm\n",
    "import model.model as module_arch\n",
    "import data_loader.data_loaders as module_data\n",
    "import model.loss as module_loss\n",
    "import model.metric as module_metric\n",
    "from model.model import AffineVAE\n",
    "from data_loader.data_loaders import make_generators_MNIST, make_generators_MNIST_CTRNFS\n",
    "\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_loaders_config(PATH, old_gpu='cuda:0', new_gpu='cuda:1'):\n",
    "    \"\"\"PATH: path to dir where training results of a run are saved\"\"\"\n",
    "    PATH = Path(PATH)\n",
    "    config_loc = PATH / 'config.json'\n",
    "    weight_path = PATH / 'model_best.pth'\n",
    "    config = json.load(open(config_loc))\n",
    "    \n",
    "    \n",
    "    def get_instance(module, name, config, *args):\n",
    "        return getattr(module, config[name]['type'])(*args, **config[name]['args'])\n",
    "\n",
    "    data_loader = get_instance(module_data, 'data_loader', config)['train']\n",
    "    valid_data_loader = get_instance(module_data, 'data_loader', config)['val']\n",
    "    model = get_instance(module_arch, 'arch', config)\n",
    "    model = model.to(torch.device(new_gpu))\n",
    "    checkpoint = torch.load(weight_path, map_location={'cuda:0': 'cuda:1'})\n",
    "    state_dict = checkpoint['state_dict']\n",
    "    \n",
    "    if config['n_gpu'] > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    model.load_state_dict(state_dict)\n",
    "    model = model.to(device).eval()\n",
    "    \n",
    "    loss_fn = get_instance(module_loss, 'loss', config)\n",
    "    metric_fns = [getattr(module_metric, met) for met in config['metrics']]\n",
    "    \n",
    "    return model, data_loader, valid_data_loader, loss_fn, metric_fns, config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rotate_mnist_batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-c3ca2ccbdccc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mrot_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrotate_mnist_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_rotation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m45\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mrot_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrot_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maffine_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrot_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rotate_mnist_batch' is not defined"
     ]
    }
   ],
   "source": [
    "config_loc = '/media/rene/data/equivariance/mnist/vae_mnist_L16/0129_230250'\n",
    "VAE, data_loader, valid_data_loader, loss_fn, metric_fns, config = get_model_loaders_config(config_loc, old_gpu='cuda:1', new_gpu='cuda:0')\n",
    "VAE = VAE.to(device)\n",
    "\n",
    "AffineVAE = getattr(module_arch, 'AffineVAE')\n",
    "affine_model = AffineVAE(pre_trained_VAE=VAE, img_size=28, input_dim=1, output_dim=1, latent_size=8, use_STN=False)\n",
    "affine_model = affine_model.to(device)\n",
    "\n",
    "files_dict_loc = '/media/rene/data/MNIST/files_dict.pkl'\n",
    "data_loaders = make_generators_MNIST_CTRNFS(files_dict_loc, batch_size=1, num_workers=4, \n",
    "                                            return_size=28, rotation_range=None, normalize=False)\n",
    "\n",
    "\n",
    "def vae_loss_unreduced(output, target, KLD_weight=1):\n",
    "    recon_x, mu_logvar  = output\n",
    "    mu = mu_logvar[:, 0:int(mu_logvar.size()[1]/2)]\n",
    "    logvar = mu_logvar[:, int(mu_logvar.size()[1]/2):]\n",
    "    KLD = -0.5 * torch.sum(1 + 2 * logvar - mu.pow(2) - (2 * logvar).exp(), dim=1)\n",
    "    BCE = F.mse_loss(recon_x, target, reduction='none')    \n",
    "    BCE = torch.sum(BCE, dim=(1, 2, 3))\n",
    "    loss = BCE + KLD_weight*KLD\n",
    "    return loss\n",
    "\n",
    "\n",
    "data, target = next(iter(data_loader))\n",
    "batch_size = data.shape[0]\n",
    "rot_x = rotate_mnist_batch(data, return_size=40, fixed_rotation=45)\n",
    "rot_x, target = rot_x.to(device), target.to(device)\n",
    "output = affine_model(rot_x, deterministic=True)\n",
    "loss = vae_loss_unreduced(output, rot_x)\n",
    "loss.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HVAE",
   "language": "python",
   "name": "hvae"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
