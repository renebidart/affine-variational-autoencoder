{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test out VAEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "from IPython.display import display, HTML\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# Nicer way to import the module?\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "from utils.display import read_img_to_np, torch_to_np\n",
    "from utils.norms import MNIST_norm\n",
    "import model.model as module_arch\n",
    "import data_loader.data_loaders as module_data\n",
    "import model.loss as module_loss\n",
    "import model.metric as module_metric\n",
    "device = torch.device(\"cuda:1\")\n",
    "\n",
    "# from utils.loading import load_net\n",
    "# from utils.data import make_generators_DF\n",
    "# from utils.train_val import validate_epoch\n",
    "# from utils.evaluation import evaluate_adv_files_df, get_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_loaders_config(PATH, old_gpu='cuda:0', new_gpu='cuda:1'):\n",
    "    \"\"\"PATH: path to dir where training results of a run are saved\"\"\"\n",
    "    PATH = Path(PATH)\n",
    "    config_loc = PATH / 'config.json'\n",
    "    weight_path = PATH / 'model_best.pth'\n",
    "    config = json.load(open(config_loc))\n",
    "    \n",
    "    \n",
    "    def get_instance(module, name, config, *args):\n",
    "        return getattr(module, config[name]['type'])(*args, **config[name]['args'])\n",
    "\n",
    "    data_loader = get_instance(module_data, 'data_loader', config)['train']\n",
    "    valid_data_loader = get_instance(module_data, 'data_loader', config)['val']\n",
    "    model = get_instance(module_arch, 'arch', config)\n",
    "    model = model.to(torch.device(new_gpu))\n",
    "    checkpoint = torch.load(weight_path, map_location={'cuda:0': 'cuda:1'})\n",
    "    state_dict = checkpoint['state_dict']\n",
    "    \n",
    "    if config['n_gpu'] > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    model.load_state_dict(state_dict)\n",
    "    model = model.to(device).eval()\n",
    "    \n",
    "    loss_fn = get_instance(module_loss, 'loss', config)\n",
    "    metric_fns = [getattr(module_metric, met) for met in config['metrics']]\n",
    "    \n",
    "    return model, data_loader, valid_data_loader, loss_fn, metric_fns, config\n",
    "\n",
    "\n",
    "def display_results_auto(vae_model, config, device, rotate=0, \n",
    "                         norm=None, num_samples=3, data='bw', size = 28, label_col_name='label', save_loc=None):\n",
    "    with torch.cuda.device(device.index): # ??? Why the fuck???        \n",
    "        files_dict_loc = config['data_loader']['args']['files_dict_loc']\n",
    "        with open(files_dict_loc, 'rb') as f:\n",
    "            files_df = pickle.load(f)['train']\n",
    "            \n",
    "        if label_col_name:\n",
    "            all_labels = files_df[label_col_name].unique()\n",
    "        else:\n",
    "            all_labels = list(range(num_samples))\n",
    "        row_names = []\n",
    "        col_names = ['Original', \"Reconstructed\"]\n",
    "\n",
    "        fig, ax = plt.subplots(num_samples, 2, sharex='col', sharey='row',figsize=(10,10))\n",
    "\n",
    "        for i, label in enumerate(all_labels[0:num_samples]):\n",
    "            if label_col_name:\n",
    "                sample_df = files_df.loc[files_df[label_col_name] == label].sample(n=1)\n",
    "                label = sample_df[label_col_name].iloc[0]\n",
    "                row_names.append(label)\n",
    "            else:\n",
    "                sample_df = files_df.sample(n=1)\n",
    "            img_path = sample_df['path'].iloc[0]\n",
    "\n",
    "            if data == 'bw': # Assume MNIST\n",
    "                img = read_img_to_np(img_path, bw=True)\n",
    "                transform = transforms.Compose([\n",
    "                                                transforms.RandomRotation((rotate, rotate), expand=True),\n",
    "                                                transforms.Resize(size),\n",
    "                                                transforms.Pad((6, 6)),\n",
    "                                                transforms.ToTensor(),\n",
    "                                                transforms.Normalize(*MNIST_norm)])\n",
    "            else:\n",
    "                img = read_img_to_np(img_path, bw=False, size=size)\n",
    "                transform = transforms.Compose([\n",
    "                                                transforms.RandomRotation((rotate, rotate), expand=True),\n",
    "                                                transforms.Resize(size),\n",
    "                                                transforms.ToTensor(),\n",
    "                                                transforms.Normalize(*norm)])\n",
    "\n",
    "            tensor_img = transform(Image.open(img_path)).unsqueeze(0).to(device)\n",
    "            tensor_label = torch.from_numpy(np.array(label)).unsqueeze(0).type(torch.LongTensor).to(device)\n",
    "\n",
    "            output = model(tensor_img,  deterministic=False)\n",
    "            recon_x = output[0]\n",
    "            \n",
    "            ax[i, 0].imshow(torch_to_np(tensor_img), cmap='Greys',  interpolation='nearest')\n",
    "            ax[i, 1].imshow(torch_to_np(recon_x), cmap='Greys',  interpolation='nearest')\n",
    "            ax[i, 0].axis('off')\n",
    "            ax[i, 1].axis('off')\n",
    "\n",
    "        for curr_ax, col in zip(ax[0], col_names):\n",
    "            curr_ax.set_title(col)\n",
    "        if label_col_name:\n",
    "            for curr_ax, row in zip(ax[:,0], row_names):\n",
    "                curr_ax.set_ylabel(row, rotation=0, size='large')\n",
    "        if save_loc:\n",
    "            plt.savefig(save_loc, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST - Check standard autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [1 x 1600], m2: [784 x 32] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:249",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-832b65823140>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mPATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/media/rene/data/equivariance/mnist/vae_mnist_L32/0114_133313'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_fns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model_loaders_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda:1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda:1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdisplay_results_auto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrotate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_col_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_loc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-95-4a121a92d3d5>\u001b[0m in \u001b[0;36mdisplay_results_auto\u001b[0;34m(vae_model, config, device, rotate, norm, num_samples, data, size, label_col_name, save_loc)\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mtensor_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_img\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m             \u001b[0mrecon_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/rene/ADV/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/rene/code/equivariance/model/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, deterministic)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mmu_logvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreparameterize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu_logvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mrecon_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/rene/code/equivariance/model/model.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc_conv4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mmu_logvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_mu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_logvar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmu_logvar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/rene/ADV/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/rene/ADV/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/rene/ADV/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [1 x 1600], m2: [784 x 32] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:249"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PATH = '/media/rene/data/equivariance/mnist/vae_mnist_L32/0114_133313'\n",
    "model, data_loader, valid_data_loader, loss_fn, metric_fns, config = get_model_loaders_config(PATH, old_gpu='cuda:1', new_gpu='cuda:1')\n",
    "display_results_auto(model, config, device, rotate=0, num_samples=3, label_col_name='class', save_loc=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perfromance on rotated images\n",
    "* Test the performance on rotated samples \n",
    "* Model was not trained with rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vae_perf(model, data_loader, loss_fn, metric_fns, device, rotation=None, resized_crop=None):\n",
    "    \"\"\"For a vae, not cvae. for mnist\"\"\"\n",
    "    with torch.cuda.device(device.index):\n",
    "        img_size = 28\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "\n",
    "        total_loss = 0.0\n",
    "        total_metrics = torch.zeros(len(metric_fns))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, (data, target) in enumerate(tqdm(data_loader)):\n",
    "                batch_size = data.shape[0]\n",
    "                \n",
    "                # Do transforms (transforms done in PIL)\n",
    "                for i in range(batch_size):\n",
    "                    img = TF.to_pil_image(data[i, :, :])\n",
    "#                     img = TF.affine(img, rotation, translate=(0, 0), scale=1, shear=0)\n",
    "                    if rotation:\n",
    "                        img = TF.rotate(img, rotation)\n",
    "                    if resized_crop:\n",
    "                        i = (img_size - 28)/2 \n",
    "                        img = TF.resized_crop(img, 4, 4, resized_crop, resized_crop, (img_size, img_size))\n",
    "                    data[i, :, :] = TF.to_tensor(img)\n",
    "                \n",
    "                data, target = data.to(device), target.to(device)                \n",
    "                output = model(data)\n",
    "\n",
    "                # computing loss, metrics on test set\n",
    "                loss = loss_fn(output, data)\n",
    "                total_loss += loss.item() * batch_size\n",
    "                for i, metric in enumerate(metric_fns):\n",
    "                    total_metrics[i] += metric(output, data) * batch_size\n",
    "\n",
    "        n_samples = len(data_loader.sampler)\n",
    "        log = {'loss': total_loss / n_samples}\n",
    "        log.update({met.__name__ : total_metrics[i].item() / n_samples for i, met in enumerate(metric_fns)})\n",
    "        print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rotation_aug' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-abb358d1a06e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mPATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/media/rene/data/equivariance/mnist/vae_mnist_L32/0114_133313'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_fns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model_loaders_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda:1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda:1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mget_vae_perf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_fns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrotation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-95-4a121a92d3d5>\u001b[0m in \u001b[0;36mget_model_loaders_config\u001b[0;34m(PATH, old_gpu, new_gpu)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'args'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mdata_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data_loader'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mvalid_data_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data_loader'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_arch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'arch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-95-4a121a92d3d5>\u001b[0m in \u001b[0;36mget_instance\u001b[0;34m(module, name, config, *args)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'args'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mdata_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data_loader'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/rene/code/equivariance/data_loader/data_loaders.py\u001b[0m in \u001b[0;36mmake_generators_MNIST\u001b[0;34m(files_dict_loc, batch_size, num_workers, img_size, path_colname, label_colname, label, return_loc)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mfiles_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexpected_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0mdesired_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpected_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mdelta_width\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdesired_size\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rotation_aug' is not defined"
     ]
    }
   ],
   "source": [
    "PATH = '/media/rene/data/equivariance/mnist/vae_mnist_L32/0114_133313'\n",
    "model, data_loader, valid_data_loader, loss_fn, metric_fns, config = get_model_loaders_config(PATH, old_gpu='cuda:1', new_gpu='cuda:1')\n",
    "\n",
    "get_vae_perf(model, valid_data_loader, loss_fn, metric_fns, device, rotation=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:01<00:00, 21.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 65856.3617078125, 'BCE': 55247.872, 'KLD': 10608.4872}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "get_vae_perf(model, valid_data_loader, loss_fn, metric_fns, device, rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:01<00:00, 22.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 65607.2440203125, 'BCE': 54976.5824, 'KLD': 10630.6712}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "get_vae_perf(model, valid_data_loader, loss_fn, metric_fns, device, rotation=180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:01<00:00, 21.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 65896.17305625, 'BCE': 55285.6768, 'KLD': 10610.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "get_vae_perf(model, valid_data_loader, loss_fn, metric_fns, device, rotation=-90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:01<00:00, 21.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 53047.013478515626, 'BCE': 41819.424, 'KLD': 11227.5928}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "get_vae_perf(model, valid_data_loader, loss_fn, metric_fns, device, rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of data_loader.data_loaders failed: Traceback (most recent call last):\n",
      "  File \"/media/rene/ADV/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/media/rene/ADV/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 368, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/home/rene/miniconda3/lib/python3.6/imp.py\", line 315, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"/home/rene/miniconda3/lib/python3.6/importlib/__init__.py\", line 166, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 618, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 674, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 781, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 741, in source_to_code\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/media/rene/code/equivariance/data_loader/data_loaders.py\", line 83\n",
      "    transfroms.RandomResizedCrop(size=img_size, scale=(img_size, ) expand=True),\n",
      "                                                                        ^\n",
      "SyntaxError: invalid syntax\n",
      "]\n",
      "[autoreload of data_loader.data_loaders failed: Traceback (most recent call last):\n",
      "  File \"/media/rene/ADV/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/media/rene/ADV/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 368, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/home/rene/miniconda3/lib/python3.6/imp.py\", line 315, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"/home/rene/miniconda3/lib/python3.6/importlib/__init__.py\", line 166, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 618, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 674, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 781, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 741, in source_to_code\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/media/rene/code/equivariance/data_loader/data_loaders.py\", line 83\n",
      "    transfroms.RandomResizedCrop(size=img_size, scale=(img_size, ) expand=True),\n",
      "                                                                        ^\n",
      "SyntaxError: invalid syntax\n",
      "]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7142857142857143"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5/7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ADV",
   "language": "python",
   "name": "adv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
